{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo, haremos uso de la API de Twitter para extraer tweets y construir nuestro datase de alrededor de 100 mil tweets, de colombia en la última semana.\n",
    "\n",
    "Primero, importamos librerías que nos serán útil para manejar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# For sending GET requests from the API\n",
    "import requests\n",
    "\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo definimos funciones de autorización y todo lo necesario para crear urls y endpoints a la API de twitter, de tal manera que sea fácil de manejar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    # Twitter API token goes here\n",
    "    return  \"XXXX-XXXX-XXXX-XXXX\"\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\" #Change to the endpoint you want to collect data from\n",
    "    # search_url = \"https://api.twitter.com/2/tweets/search/all\" # With an academic research access\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "                    \n",
    "    return (search_url, query_params)\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    #params object received from create_url function\n",
    "    params['next_token'] = next_token\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función helper, que dado un archivo csv, le añade la respuesta de la API al archivo, con los atributos que nos interesan, principalmente: `text`, y `retweet_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        #if ('geo' in tweet):   \n",
    "        #    geo = tweet['geo']['place_id']\n",
    "        #else:\n",
    "        #    geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        # remove geo, not neccesary\n",
    "        #res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        res = [author_id, created_at, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos tweets (no retweets) no repetidos en español publicados en Colombia. Tomaremos los tweets del 15 de Febrero hasta al 21 de Febrero. Nuestro objetivo es tomar aproximadamente 100000 tweets durante el periodo anteriormente mencionado, teniendo en cuenta las condiciones y restricciones que la API de Twitter nos proporciona:\n",
    "\n",
    "- Máximo 450 peticiones por 15 minutos.  \n",
    "- Máximo 100 tweets por petición.  \n",
    "- Máximo de 500.000 consultas de tweets (por mes). \n",
    "\n",
    "De esta manera se apunta a obtener 100.000 tweets, almenos 14286 tweets por cada día.\n",
    "\n",
    "Para esto se define la siguiente función, que nos ayuda a recoger los tweets por periodo, donde cada período es un día de la semana mencionada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"Colombia lang:es -is:retweet\"\n",
    "\n",
    "month = 2 # feb\n",
    "day_start = 15\n",
    "day_end = 21\n",
    "start_list = [f'2022-02-{i}T00:00:00.000Z' for i in range(day_start,day_end + 1)]\n",
    "end_list = [f'2022-02-{i+1}T00:00:00.000Z' for i in range(day_start,day_end + 1)]\n",
    "max_results = 100\n",
    "\n",
    "\n",
    "def gatherTweetsPerPeriod(start_time, end_time, filename, desired_tweets = 600 ):\n",
    "\n",
    "    #Total number of tweets we collected from the loop\n",
    "    collected = 0\n",
    "\n",
    "    # Create file\n",
    "    csvFile = open(filename, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "    csvWriter.writerow(['author id', 'created_at', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "    csvFile.close()\n",
    "\n",
    "    # Inputs\n",
    "    #count = 0 # Counting tweets per time period\n",
    "    #max_count = 500 # Max tweets per time period\n",
    "    # Check if max_count reached\n",
    "    max_results = min(desired_tweets, 100)\n",
    "    next_token = None\n",
    "    while collected < desired_tweets:\n",
    "        try:\n",
    "            url = create_url(keyword, start_time, end_time, max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token=next_token)\n",
    "            result_count = json_response['meta']['result_count']\n",
    "        \n",
    "            since_id = json_response['meta']['newest_id']\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            append_to_csv(json_response, filename)\n",
    "            collected += result_count\n",
    "\n",
    "            # Save the token to use for next call\n",
    "            # next_token = json_response['meta']['next_token']\n",
    "            time.sleep(1)\n",
    "        except Exception as error:\n",
    "            print(f\"Last Query Newest ID: {since_id}\")\n",
    "            print(\"Caught Exception:\")\n",
    "            print(error)\n",
    "            break\n",
    "    print(f\"Last Query Newest ID: {since_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último corremos dicha función por cada periodo (No se puede ejecutar todo en uno, dada los límites de twitter)\n",
    "\n",
    "Y así lentamente creamos nuestro dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering 14286 tweets for period 2022-02-21T00:00:00.000Z-2022-02-22T00:00:00.000Z (period 6)\n",
      "Endpoint Response Code: 401\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'since_id' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-89c149a53eed>\u001b[0m in \u001b[0;36mgatherTweetsPerPeriod\u001b[1;34m(start_time, end_time, filename, desired_tweets)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mjson_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnect_to_endpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mresult_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'meta'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result_count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-981a5958f2a8>\u001b[0m in \u001b[0;36mconnect_to_endpoint\u001b[1;34m(url, headers, params, next_token)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: (401, '{\\n  \"title\": \"Unauthorized\",\\n  \"type\": \"about:blank\",\\n  \"status\": 401,\\n  \"detail\": \"Unauthorized\"\\n}')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b1eb765fb97d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Gathering {per_day} tweets for period {start_list[period]}-{end_list[period]} (period {period})\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgatherTweetsPerPeriod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperiod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperiod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdesired_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mper_day\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-89c149a53eed>\u001b[0m in \u001b[0;36mgatherTweetsPerPeriod\u001b[1;34m(start_time, end_time, filename, desired_tweets)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Last Query Newest ID: {since_id}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Caught Exception:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'since_id' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import os\n",
    "\n",
    "\n",
    "total_target = int(1e5)\n",
    "per_day = ceil(total_target / 7)\n",
    "period = 6 # index from 0-6 \n",
    "filename = f\"data/period{period}.csv\" #remove previously existing file\n",
    "if os.path.exists(filename):\n",
    "  os.remove(filename)\n",
    "print(f\"Gathering {per_day} tweets for period {start_list[period]}-{end_list[period]} (period {period})\")\n",
    "gatherTweetsPerPeriod(start_list[period],end_list[period],desired_tweets = per_day, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último verificamos nuestros tweets obtenidos, verificando que sean únicos, tengamos la cantidad deseada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tweets?: True\n",
      "100604\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1493736882106245120</td>\n",
       "      <td>es</td>\n",
       "      <td>@JuanCar99077589 @elojodiestro @intiasprilla Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1493736869108006912</td>\n",
       "      <td>es</td>\n",
       "      <td>@NairoQuinCo no tiene ni puta idea como es la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1493736868755611649</td>\n",
       "      <td>es</td>\n",
       "      <td>@Enrique_GomezM  deje me  decirle con todo res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1493736858999787521</td>\n",
       "      <td>es</td>\n",
       "      <td>@PizarroMariaJo @MovimientoMAIS @UP_Colombia @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1493736858769108992</td>\n",
       "      <td>es</td>\n",
       "      <td>Mal día para el duquecito en el parlamento eur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id lang                                              tweet\n",
       "0  1493736882106245120   es  @JuanCar99077589 @elojodiestro @intiasprilla Y...\n",
       "1  1493736869108006912   es  @NairoQuinCo no tiene ni puta idea como es la ...\n",
       "2  1493736868755611649   es  @Enrique_GomezM  deje me  decirle con todo res...\n",
       "3  1493736858999787521   es  @PizarroMariaJo @MovimientoMAIS @UP_Colombia @...\n",
       "4  1493736858769108992   es  Mal día para el duquecito en el parlamento eur..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.concat([pd.read_csv(f'data/period{i}.csv') for i in range(7)])\n",
    "df = df[[\"id\",\"lang\",\"tweet\"]]\n",
    "unique = df[\"id\"].is_unique\n",
    "print(f\"Unique tweets?: {unique}\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo así el conjunto de datos listo para ser procesado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
